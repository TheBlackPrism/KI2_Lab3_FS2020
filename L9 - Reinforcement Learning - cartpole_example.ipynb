{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Gym Cartpole-v1 Beispiel\n",
    "\n",
    "Dieses Beispiel basiert auf dem offiziellen OpenAI Gym [Tutorial](https://gym.openai.com/docs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym  # If gym isn't installed yet in your environment, then you can install it via pip: pip install gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Führt die nächste Zelle aus, um zu testen, dass alles funktioniert.\n",
    "Es sollte ein Fenster erscheinen, das für ein paar Sekunden das CartPole\n",
    "Environment zeigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yves\\gym\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n",
    "for _ in range(200):\n",
    "    env.render()\n",
    "    observation, reward, done, info = env.step(env.action_space.sample())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environments\n",
    "\n",
    "Die Hauptkomponenten von OpenAI Gym sind die verschiedenen Environments. Die Funktion\n",
    "```gym.make(environment_name)``` erstellt ein neues Environment.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jedes Environment hat einen Observation-Space und einen Action-Space. Der Observation-Space beschreibt, welche Arten von Observationen euer Agent machen kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(4,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Box(4,)``` heisst, dass alle Observationen, die euerem System zu Verfügung stehen 4-dimensional sind. Den Bereich der Dimensionen kann mit ```env.observation_space.low``` und\n",
    "```env.observation_space.high``` eingesehen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Fall des Cartpole-Environments stehen die 4 Dimensionen für:\n",
    "* Dim 0: Position des Wagens\n",
    "* Dim 1: Geschwindigkeit des Wagens\n",
    "* Dim 2: Winkel des Pendels\n",
    "* Dim 3: Winkelgeschwindigkeit des Pendels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Action-Space beschreibt, welche Aktionen ein Agent im Environment vornehmen kann. Das heisst, welchen Input ```env.step(action)``` erwartet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der CartPole Action-Space ist ```Discrete(2)```. \n",
    "```Discrete(n)``` heisst, das Environment erwartet eine von n discreten Aktionen als Input erwarted: ```0 <= action < n```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Fall des CartPole Environments wird mit 0 und 1 bestimmt, ob der Wagen nach links oder rechts verschoben wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wichtigste Funktionen\n",
    "\n",
    "* ```env = gym.make('CartPole-v1')``` erstellt ein neues CartPole Environment\n",
    "* ```observation = env.reset()``` initialisiert das Environment in einen neuen Startzustand und gibt die erste Observation für euren Agenten zurück.\n",
    "* ```observation, reward, done, info = env.step(action)``` Update Funktion des Environments. Gibt neue Observation und die Belohnung zurück. Wenn ```done``` auf ```True``` gesetzt ist, befindet sich das Environment in einem Endzustand und sollte mit ```env.reset()``` zurückgesetzt werden. ```info``` enthält zusätzliche Informationen, sollte für CartPole leer sein.\n",
    "* ```env.render()``` zeigt eine graphische Representation des Zustands des Environments an.\n",
    "* ```env.close()``` beendet das Environment. Je nach Konfiguration eures Computers, muss dies nach ```env.render()``` aufgerufen werden, damit sich das Fenster schliesst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modell\n",
    "\n",
    "In dieser Einführung verwenden wir ein sehr simples Kontroller-Modell und optimisieren es mit  2 verschiedenen Random-Search Varianten.\n",
    "\n",
    "Als Modellparameter verwenden wir $\\theta \\in \\mathcal{R}^4$ und die nächste Aktion wählen wir gemäss Vorzeichen des Skalarprodukts zwischen Parametervektor und Beobachtungsvektor:\n",
    "\\begin{equation*}\n",
    "    action =\n",
    "    \\begin{cases}\n",
    "        0 & \\theta^T x_{obs} < 0 \\\\\n",
    "        1 & else\n",
    "    \\end{cases}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simpler Training Loop für naiven Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "best_param = np.zeros(4)\n",
    "best_cumulative_reward = 0.0\n",
    "for epoch in range(2000):\n",
    "    observation = env.reset()\n",
    "    param = 2 * np.random.rand(4) - 1 # initialize new random parameter\n",
    "    done = False\n",
    "    cumulative_reward = 0.0\n",
    "    while not done:\n",
    "        # choose action according to our model\n",
    "        action = 0 if np.dot(param, observation) < 0.0 else 1\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        cumulative_reward += reward\n",
    "    \n",
    "    if cumulative_reward > best_cumulative_reward:\n",
    "        best_param = param\n",
    "        best_cumulative_reward = cumulative_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisierung des gelernten Verhaltens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "done = False\n",
    "observation = env.reset()\n",
    "cum_reward = 0.0\n",
    "while not done:\n",
    "    env.render()\n",
    "    action = 0 if np.dot(best_param, observation) < 0.0 else 1\n",
    "    observation, reward, done, _  = env.step(action)\n",
    "    cum_reward += reward\n",
    "env.close()\n",
    "print(cum_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hill-Climbing\n",
    "\n",
    "Das folgende Beispiel wurde von [hier](http://kvfrans.com/simple-algoritms-for-solving-cartpole/) übernommen. Anstatt nach jeder Iteration den Parametervektor komplett neu zu wählen addieren wir eine kleine zufällige Veränderung zum besten bisherigen Parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir haben die Logik für unseren Agenten in eine separate Klasse verschoben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HillClimbRandomPolicy(object):\n",
    "    def __init__(self, dim=4):\n",
    "        self.dim = dim\n",
    "        self.param = self._resample()\n",
    "        self.max_reward = 0.0\n",
    "        self.epsilon = 0.1\n",
    "        self.best_param = self.param\n",
    "        \n",
    "    def _resample(self):\n",
    "        return 2 * np.random.rand(self.dim) - 1\n",
    "    \n",
    "    def action(self, observation):\n",
    "        return 0 if np.dot(observation, self.param) < 0.0 else 1\n",
    "    \n",
    "    def best_action(self, observation):\n",
    "        return 0 if np.dot(observation, self.best_param) < 0.0 else 1\n",
    "    \n",
    "    def update(self, history):\n",
    "        total_reward = np.sum([h['reward'] for h in history])\n",
    "        if total_reward > self.max_reward:\n",
    "            self.max_reward = total_reward\n",
    "            self.best_param = self.param\n",
    "        else:\n",
    "            self.param = self.best_param\n",
    "        self.param += self.epsilon * self._resample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der folgende Trainingloop ist sehr generisch. Obwohl nicht strikt notwendig für unseren simplen Agenten, sammeln wir den kompletten Verlauf aller Aktionen, Observationen und Belohnungen, da viele cleverere Algorithmen diese Information brauchen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "policy = HillClimbRandomPolicy()\n",
    "for epoch in range(30000):\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    history = []\n",
    "    while not done:\n",
    "        action = policy.action(observation)\n",
    "        history_elem = {}\n",
    "        history_elem['observation'] = observation\n",
    "        history_elem['action'] = action\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        history_elem['reward'] = reward\n",
    "        \n",
    "        history.append(history_elem)\n",
    "\n",
    "    policy.update(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisiert das gelernte verhalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "done = False\n",
    "observation = env.reset()\n",
    "cum_reward = 0.0\n",
    "while not done:\n",
    "    env.render()\n",
    "    action = policy.best_action(observation)\n",
    "    observation, reward, done, _  = env.step(action)\n",
    "    cum_reward += reward\n",
    "env.close()\n",
    "print(cum_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ende\n",
    "\n",
    "Das CartPole Environment gilt als gelöst, wenn der Agent in 100 konsekutiven Runs eine durchschnittliche Belohnung von 200 erreicht.\n",
    "Viel Spass!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
